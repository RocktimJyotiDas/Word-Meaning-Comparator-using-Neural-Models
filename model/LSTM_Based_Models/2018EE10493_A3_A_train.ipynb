{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2018EE10493_A3_A_train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "YjxmArzemaB_",
        "2dJp9EcU5Hag"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWjwfx8T5OAJ"
      },
      "source": [
        "Make a copy of this notebook and rename using your USERID in the following format, 2017CSZ8058\n",
        "\n",
        "Give read access to keshavkolluru@gmail.com, vishalsaley114@gmail.com and kartikeya.badola@gmail.com\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spAvH1fF0Rhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e2023e4-7c88-4c18-e3b1-49f8382a5e50"
      },
      "source": [
        "## DONT CHANGE THIS CELL\n",
        "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-11 13:08:41--  http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
            "Resolving www.cse.iitd.ac.in (www.cse.iitd.ac.in)... 103.27.9.152\n",
            "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip [following]\n",
            "--2021-10-11 13:08:42--  https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
            "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 217313 (212K) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>] 212.22K   101KB/s    in 2.1s    \n",
            "\n",
            "2021-10-11 13:08:45 (101 KB/s) - ‘data.zip’ saved [217313/217313]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiV2DDbAcCxq"
      },
      "source": [
        "## Import relevant packages\n",
        "\n",
        "import os\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "import torch.optim as O\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "import logging\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from pdb import set_trace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXPSu6e7GHQn"
      },
      "source": [
        "import zipfile\n",
        "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To40gAscF_QC"
      },
      "source": [
        "torch.manual_seed(500)\n",
        "torch.cuda.manual_seed(500)\n",
        "np.random.seed(500)\n",
        "random.seed(500)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.deterministic = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3OfnG4RERqE"
      },
      "source": [
        "with zipfile.ZipFile(\"./data.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"./NLP_A3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFal9P9GEUgO"
      },
      "source": [
        "training_data_path = \"./NLP_A3/data/train/train.data.txt\"\n",
        "validation_data_path = \"./NLP_A3/data/validation/validation.data.txt\"\n",
        "training_data_predict_path = \"./NLP_A3/data/train/train.gold.txt\"\n",
        "validation_data_predict_path = \"./NLP_A3/data/validation/validation.gold.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsGd3PevvF0H"
      },
      "source": [
        "\"\"\"checking arguments\"\"\"\n",
        "def check_args(args):\n",
        "\t# --result_dir\n",
        "\tcheck_folder(os.path.join(args['results_dir'], args['model'], args['dataset']))\n",
        "\n",
        "\t# --epoch\n",
        "\ttry:\n",
        "\t\t\tassert args['epochs'] >= 1\n",
        "\texcept:\n",
        "\t\t\tprint('number of epochs must be larger than or equal to one')\n",
        "\n",
        "\t# --batch_size\n",
        "\ttry:\n",
        "\t\t\tassert args['batch_size'] >= 1\n",
        "\texcept:\n",
        "\t\t\tprint('batch size must be larger than or equal to one')\n",
        "\treturn args\n",
        "\n",
        "def get_device(gpu_no):\n",
        "\tif torch.cuda.is_available():\n",
        "\t\ttorch.cuda.set_device(gpu_no)\n",
        "\t\treturn torch.device('cuda:{}'.format(gpu_no))\n",
        "\telse:\n",
        "\t\treturn torch.device('cpu')\n",
        "\n",
        "def makedirs(name):\n",
        "\t\"\"\"helper function for python 2 and 3 to call os.makedirs()\n",
        "\t\tavoiding an error if the directory to be created already exists\"\"\"\n",
        "\n",
        "\timport os, errno\n",
        "\n",
        "\ttry:\n",
        "\t\tos.makedirs(name)\n",
        "\texcept OSError as ex:\n",
        "\t\tif ex.errno == errno.EEXIST and os.path.isdir(name):\n",
        "\t\t\t# ignore existing directory\n",
        "\t\t\tpass\n",
        "\t\telse:\n",
        "\t\t\t# a different error happened\n",
        "\t\t\traise\n",
        "\n",
        "def check_folder(log_dir):\n",
        "\tif not os.path.exists(log_dir):\n",
        "\t\tos.makedirs(log_dir)\n",
        "\treturn log_dir\n",
        "\n",
        "def get_logger(args, phase):\n",
        "\tlogging.basicConfig(level=logging.INFO, \n",
        "\t\t\t\t\t\t\t\t\t\t\t\tfilename = \"{}/{}/{}/{}.log\".format(args['results_dir'], args['model'], args['dataset'], phase),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tformat = '%(asctime)s - %(message)s', \n",
        "\t\t\t\t\t\t\t\t\t\t\t\tdatefmt='%d-%b-%y %H:%M:%S')\n",
        "\treturn logging.getLogger(phase)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ewWOkuFduto"
      },
      "source": [
        "def Dataset(batch_size):\n",
        "  file_ = pd.read_csv(training_data_path,sep='\\t', header = None, names = ['word', 'POS', 'index_sent1', 'sent1', 'sent2'])\n",
        "  file_v = pd.read_csv(validation_data_path,sep='\\t', header = None, names = ['word', 'POS', 'index_sent1', 'sent1', 'sent2'])\n",
        "  file_['index_sent2'] = file_['index_sent1']\n",
        "  file_v['index_sent2'] = file_v['index_sent1']\n",
        "  for i in file_.index:\n",
        "    idx1 = file_.at[i, 'index_sent1']\n",
        "    file_.at[i, 'index_sent1'] = idx1[0]\n",
        "    idx2 = file_.at[i, 'index_sent2']\n",
        "    file_.at[i, 'index_sent2'] = idx2[-1]\n",
        "  for i in file_v.index:\n",
        "    idx1 = file_v.at[i, 'index_sent1']\n",
        "    file_v.at[i, 'index_sent1'] = idx1[0]\n",
        "    idx2 = file_v.at[i, 'index_sent2']\n",
        "    file_v.at[i, 'index_sent2'] = idx2[-1]\n",
        "  with open(training_data_predict_path) as f:\n",
        "    data_predict = f.readlines()\n",
        "    f.close()\n",
        "  with open(validation_data_predict_path) as f:\n",
        "    data_predict_v = f.readlines()\n",
        "    f.close()\n",
        "  file_predict = pd.DataFrame(data_predict, columns= ['category'])\n",
        "  file_predict_v = pd.DataFrame(data_predict_v, columns= ['category'])\n",
        "  for i in file_predict.index:\n",
        "    idx1 = file_predict.at[i, 'category']\n",
        "    if idx1[0] == 'T':\n",
        "      file_predict.at[i, 'category'] = 1\n",
        "    else:\n",
        "      file_predict.at[i, 'category'] = 0\n",
        "  for i in file_predict_v.index:\n",
        "    idx1 = file_predict_v.at[i, 'category']\n",
        "    if idx1[0] == 'T':\n",
        "      file_predict_v.at[i, 'category'] = 1\n",
        "    else:\n",
        "      file_predict_v.at[i, 'category'] = 0\n",
        "  file_['category'] = file_predict['category']\n",
        "  file_v['category'] = file_predict_v['category']\n",
        "  for i in file_.index:\n",
        "    idx1 = file_.at[i, 'POS']\n",
        "    if idx1[0] == 'N':\n",
        "      file_.at[i, 'POS'] = 1\n",
        "    else:\n",
        "      file_.at[i, 'POS'] = 0\n",
        "  for i in file_v.index:\n",
        "    idx1 = file_v.at[i, 'POS']\n",
        "    if idx1[0] == 'N':\n",
        "      file_v.at[i, 'POS'] = 1\n",
        "    else:\n",
        "      file_v.at[i, 'POS'] = 0\n",
        "  file_.to_json('train_pos.json', orient = 'records', lines = True)\n",
        "  file_v.to_json('valid_pos.json', orient = 'records', lines = True)\n",
        "  load_model_lemma = spacy.load('en', disable = ['parser','ner'])\n",
        "  def tokenize_lemma(text):\n",
        "    doc = load_model_lemma(text)\n",
        "    return [token.lemma_ for token in doc]\n",
        "\n",
        "  idx_sent = Field(sequential=False, use_vocab=False)\n",
        "  sent = Field(sequential=True, use_vocab=True, tokenize=tokenize_lemma, lower=True)\n",
        "  category = Field(sequential=False, use_vocab=False)\n",
        "  fields = {'word': ('w', sent), \"POS\":(\"pos\", category), \"index_sent1\": ('idx1', idx_sent), \"index_sent2\": ('idx2', idx_sent), \"sent1\":('s1', sent), \"sent2\":('s2',sent), \"category\":(\"cate\", category)}\n",
        "  train_data, valid_data = TabularDataset.splits(path=\"\", train=\"train_pos.json\", validation=\"valid_pos.json\", format=\"json\", fields=fields)\n",
        "  sent.build_vocab(train_data,vectors=\"glove.6B.300d\")\n",
        "  train_iterator, valid_iterator = BucketIterator.splits((train_data,valid_data), batch_size=batch_size, device=\"cuda\", sort_key = lambda x: len(x.s1), sort_within_batch=True)\n",
        "  return train_iterator, valid_iterator, sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErjoaBebdTo_"
      },
      "source": [
        "def adjust_dim(word_idx, hidden_size):\n",
        "  word_idx = torch.unsqueeze(word_idx, 0)\n",
        "  word_idx = torch.unsqueeze(word_idx, 2)\n",
        "  word_idx = torch.repeat_interleave(word_idx, hidden_size, dim=2)\n",
        "  return word_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0YxZGO_BuUA"
      },
      "source": [
        "class Context_word_embedding_model(nn.Module):\n",
        "  def __init__(self, input_size, embed_size, hidden_size, num_layers, bidirectional, num_direction, dropout, device):\n",
        "    super(Context_word_embedding_model, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.num_direction = num_direction\n",
        "    self.embedding = nn.Embedding(input_size, embed_size,padding_idx= 1)\n",
        "    self.dimension = hidden_size*num_direction\n",
        "    self.device = device\n",
        "    self.drop = nn.Dropout(p =0.1)\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, bidirectional = bidirectional, dropout = dropout)\n",
        "    self.fc = nn.Linear(self.dimension, self.dimension)\n",
        "    self.fc_pos = nn.Linear(self.dimension, 1)\n",
        "    self.cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
        "    self.sig = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, batch):\n",
        "    x = batch.s1.to(device=self.device)\n",
        "    s2 = batch.s2.to(device=self.device)\n",
        "    word_idx = batch.idx1.to(device=self.device)\n",
        "    word_idx2 = batch.idx2.to(device=self.device)\n",
        "    h0 = torch.zeros(self.num_layers*self.num_direction, x.size(1), self.hidden_size).to(self.device)\n",
        "    c0 = torch.zeros(self.num_layers*self.num_direction, x.size(1), self.hidden_size).to(self.device)\n",
        "    h02 = torch.zeros(self.num_layers*self.num_direction, s2.size(1), self.hidden_size).to(self.device)\n",
        "    c02 = torch.zeros(self.num_layers*self.num_direction, s2.size(1), self.hidden_size).to(self.device)\n",
        "\n",
        "    embedded_ = self.embedding(x)\n",
        "    embedded2_ = self.embedding(s2)\n",
        "    embedded = self.drop(embedded_)\n",
        "    embedded2 = self.drop(embedded2_)\n",
        "    outputs, _ = self.lstm(embedded, (h0,c0))\n",
        "    outputs2, __ = self.lstm(embedded2, (h02,c02))\n",
        "    word_idx = adjust_dim(word_idx, self.hidden_size*self.num_direction)\n",
        "    word_idx2 = adjust_dim(word_idx2, self.hidden_size*self.num_direction)\n",
        "    embed = torch.gather(outputs, 0, word_idx).squeeze()\n",
        "    embed2 = torch.gather(outputs2, 0, word_idx2).squeeze()\n",
        "    context_embedding = self.fc(embed)\n",
        "    context_embedding2 = self.fc(embed2)\n",
        "    out_sim = self.cos(context_embedding, context_embedding2)\n",
        "    out_final = self.sig(out_sim)\n",
        "    return out_final\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1S7BtExgQ1p"
      },
      "source": [
        "arg_dictionary = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzcxZbYugWZU"
      },
      "source": [
        "arg_dictionary['gpu'] = 0\n",
        "arg_dictionary['batch_size'] = 128\n",
        "arg_dictionary['embed_dim'] = 300\n",
        "arg_dictionary['d_hidden'] = 200\n",
        "arg_dictionary['dp_ratio'] = 0.2\n",
        "arg_dictionary['epochs'] = 20\n",
        "arg_dictionary['lr'] = 0.001\n",
        "arg_dictionary['combine'] = 'cat'\n",
        "arg_dictionary['results_dir'] = 'results'\n",
        "arg_dictionary['dataset'] = 'mnli'\n",
        "arg_dictionary['model'] = 'lstm'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vu38GmyWq1B4",
        "outputId": "fcb2a592-b3f2-4086-e85e-91ea18c96030"
      },
      "source": [
        "arg_dictionary['results_dir']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'results'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14EjR4tmz2x5"
      },
      "source": [
        "## Basic training loop\n",
        "\n",
        "class Train():\n",
        "\tdef __init__(self):\n",
        "\t\tprint(\"program execution start: {}\".format(datetime.datetime.now()))\n",
        "\t\tself.args = check_args(arg_dictionary)\n",
        "\t\tself.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\t\tself.logger = get_logger(self.args, \"train\")\n",
        "\t\tself.logger.info(\"Arguments: {}\".format(self.args))\n",
        "\t\t\n",
        "\t\tdataset_options = {\n",
        "\t\t\t\t\t\t\t\t\t\t\t'batch_size': self.args['batch_size'], \n",
        "\t\t\t\t\t\t\t\t\t\t\t'device': self.device\n",
        "\t\t\t\t\t\t\t\t\t\t}\n",
        "\n",
        "    ## TODO: Load your own dataset\n",
        "\t\tself.train_iter, self.valid_iter, self.sent = Dataset(dataset_options['batch_size'])\n",
        "\t\tself.input_size = len(self.sent.vocab)\n",
        "\t\tself.embedding_size = 300\n",
        "\t\tself.hidden_size = 200\n",
        "\t\tself.num_layers = 1\n",
        "\t\tself.bidirectional = False\n",
        "\t\tself.num_direction = 1\n",
        "\t\tself.dropout = 0\n",
        "    ## TODO: Load your own model\n",
        "\t\tself.model = Context_word_embedding_model(self.input_size, self.embedding_size, self.hidden_size, self.num_layers, self.bidirectional, self.num_direction, self.dropout, self.device)\n",
        "\t\tself.pretrained_embeddings = self.sent.vocab.vectors\n",
        "\t\tself.model.embedding.weight.data.copy_(self.pretrained_embeddings)\n",
        "\t\tself.model.to(self.device)\n",
        "\t\tself.criterion = nn.BCELoss()\n",
        "\t\tself.opt = O.Adam(self.model.parameters(), lr = self.args['lr'],weight_decay=2e-4)\n",
        "\t\tself.best_val_acc = 57\n",
        "\t\tself.scheduler = StepLR(self.opt, step_size=5, gamma=0.5)\n",
        "\n",
        "\t\tprint(\"resource preparation done: {}\".format(datetime.datetime.now()))\n",
        "\n",
        "\tdef result_checkpoint(self, epoch, train_loss, val_loss, train_acc, val_acc, took):\n",
        "\t\tif self.best_val_acc is None or val_acc > self.best_val_acc:\n",
        "\t\t\tself.best_val_acc = val_acc\n",
        "\t\t\ttorch.save({\n",
        "\t\t\t\t'accuracy': self.best_val_acc,\n",
        "\t\t\t\t# 'options': self.model_options,\n",
        "\t\t\t\t'model_dict': self.model.state_dict(),\n",
        "\t\t\t}, '{}/{}/{}/best-{}-{}-params.pt'.format(self.args['results_dir'], self.args['model'], self.args['dataset'], self.args['model'], self.args['dataset']))\n",
        "\t\tself.logger.info('| Epoch {:3d} | train loss {:5.2f} | train acc {:5.2f} | val loss {:5.2f} | val acc {:5.2f} | time: {:5.2f}s |'\n",
        "\t\t\t\t.format(epoch, train_loss, train_acc, val_loss, val_acc, took))\n",
        "\t\n",
        "\tdef train(self):\n",
        "\t\tself.model.train(); self.train_iter.init_epoch()\n",
        "\t\tn_correct, n_total, n_loss = 0, 0, 0\n",
        "\t\tfor batch_idx, batch in enumerate(self.train_iter):\n",
        "\t\t\tanswer = self.model(batch)\n",
        "\t\t\ttargets = batch.cate.to(device=self.device)\n",
        "\t\t\tloss = self.criterion(answer, targets.type_as(answer));x = answer > 0.5; predictions = list(map(int,x)); predictions = torch.tensor(predictions).to(self.device)\n",
        "\t\t\tn_correct += (predictions == targets).sum()\n",
        "\t\t\tself.opt.zero_grad()\n",
        "\t\t\tn_total += targets.shape[0]\n",
        "\t\t\tn_loss += loss.item()\n",
        "\t\t\t\n",
        "\t\t\tloss.backward(); self.opt.step()\n",
        "\t\ttrain_loss = n_loss/n_total\n",
        "\t\ttrain_acc = 100. * n_correct/n_total\n",
        "\t\treturn train_loss, train_acc\n",
        "\n",
        "\tdef validate(self):\n",
        "\t\tself.model.eval(); self.valid_iter.init_epoch()\n",
        "\t\tn_correct, n_total, n_loss = 0, 0, 0\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tfor batch_idx, batch in enumerate(self.valid_iter):\n",
        "\t\t\t\tanswer = self.model(batch)\n",
        "\t\t\t\ttargets = batch.cate.to(device=self.device)\n",
        "\t\t\t\tloss = self.criterion(answer, targets.type_as(answer)); x = answer > 0.5 ;predictions = list(map(int,x));predictions = torch.tensor(predictions).to(self.device)\n",
        "\t\t\t\tn_correct += (predictions == targets).sum()\n",
        "\t\t\t\tn_total += targets.shape[0]\n",
        "\t\t\t\tn_loss += loss.item()\n",
        "\n",
        "\t\t\tval_loss = n_loss/n_total\n",
        "\t\t\tval_acc = 100. * n_correct/n_total\n",
        "\t\t\treturn val_loss, val_acc\n",
        "\n",
        "\tdef execute(self):\n",
        "\t\tprint(\" [*] Training starts!\")\n",
        "\t\tprint('-' * 99)\n",
        "\t\tfor epoch in range(1, self.args['epochs']+1):\n",
        "\t\t\tstart = time.time()\n",
        "\n",
        "\t\t\ttrain_loss, train_acc = self.train()\n",
        "\t\t\tval_loss, val_acc = self.validate()\n",
        "\t\t\t#self.scheduler.step()\n",
        "\t\t\t\n",
        "\t\t\ttook = time.time()-start\n",
        "\t\t\tself.result_checkpoint(epoch, train_loss, val_loss, train_acc, val_acc, took)\n",
        "\n",
        "\t\t\tprint('| Epoch {:3d} | train loss {:5.2f} | train acc {:5.2f} | val loss {:5.2f} | val acc {:5.2f} | time: {:5.2f}s |'.format(\n",
        "\t\t\t\tepoch, train_loss, train_acc, val_loss, val_acc, took))\n",
        "\t\tself.finish()\n",
        "\n",
        "\tdef finish(self):\n",
        "\t\tself.logger.info(\"[*] Training finished!\\n\\n\")\n",
        "\t\tprint('-' * 99)\n",
        "\t\tprint(\" [*] Training finished!\")\n",
        "\t\tprint(\" [*] Please find the saved model and training log in results_dir\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dM2QDq2iz-h7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22c95967-381d-47aa-9958-99f27d52f824"
      },
      "source": [
        "## Start training\n",
        "task = Train()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "program execution start: 2021-10-11 13:08:47.393104\n",
            "resource preparation done: 2021-10-11 13:09:42.422902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYQm3Id6z92f",
        "outputId": "945c2d06-fb7c-4203-bb44-18e7213fb186"
      },
      "source": [
        "task.execute()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " [*] Training starts!\n",
            "---------------------------------------------------------------------------------------------------\n",
            "| Epoch   1 | train loss  0.01 | train acc 53.50 | val loss  0.01 | val acc 56.74 | time:  1.10s |\n",
            "| Epoch   2 | train loss  0.01 | train acc 60.34 | val loss  0.01 | val acc 55.02 | time:  0.99s |\n",
            "| Epoch   3 | train loss  0.01 | train acc 63.74 | val loss  0.01 | val acc 58.78 | time:  0.98s |\n",
            "| Epoch   4 | train loss  0.00 | train acc 67.83 | val loss  0.01 | val acc 59.09 | time:  0.99s |\n",
            "| Epoch   5 | train loss  0.00 | train acc 70.50 | val loss  0.01 | val acc 60.03 | time:  0.97s |\n",
            "| Epoch   6 | train loss  0.00 | train acc 73.54 | val loss  0.01 | val acc 58.93 | time:  0.98s |\n",
            "| Epoch   7 | train loss  0.00 | train acc 75.44 | val loss  0.01 | val acc 59.87 | time:  0.96s |\n",
            "| Epoch   8 | train loss  0.00 | train acc 77.43 | val loss  0.01 | val acc 58.62 | time:  0.96s |\n",
            "| Epoch   9 | train loss  0.00 | train acc 77.93 | val loss  0.01 | val acc 58.15 | time:  0.99s |\n",
            "| Epoch  10 | train loss  0.00 | train acc 79.73 | val loss  0.01 | val acc 58.78 | time:  0.97s |\n",
            "| Epoch  11 | train loss  0.00 | train acc 81.37 | val loss  0.01 | val acc 57.21 | time:  0.99s |\n",
            "| Epoch  12 | train loss  0.00 | train acc 82.04 | val loss  0.01 | val acc 56.58 | time:  0.97s |\n",
            "| Epoch  13 | train loss  0.00 | train acc 82.61 | val loss  0.01 | val acc 56.74 | time:  0.97s |\n",
            "| Epoch  14 | train loss  0.00 | train acc 83.16 | val loss  0.01 | val acc 57.68 | time:  0.97s |\n",
            "| Epoch  15 | train loss  0.00 | train acc 84.06 | val loss  0.01 | val acc 56.27 | time:  0.98s |\n",
            "| Epoch  16 | train loss  0.00 | train acc 83.77 | val loss  0.01 | val acc 56.58 | time:  0.96s |\n",
            "| Epoch  17 | train loss  0.00 | train acc 85.56 | val loss  0.01 | val acc 57.52 | time:  0.99s |\n",
            "| Epoch  18 | train loss  0.00 | train acc 85.61 | val loss  0.01 | val acc 54.86 | time:  0.98s |\n",
            "| Epoch  19 | train loss  0.00 | train acc 85.81 | val loss  0.01 | val acc 53.92 | time:  0.98s |\n",
            "| Epoch  20 | train loss  0.00 | train acc 86.24 | val loss  0.01 | val acc 55.49 | time:  0.97s |\n",
            "---------------------------------------------------------------------------------------------------\n",
            " [*] Training finished!\n",
            " [*] Please find the saved model and training log in results_dir\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKm7hnCMYUih",
        "outputId": "19d24082-580f-4b96-d474-934385f1fc42"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.zip  \u001b[0m\u001b[01;34mNLP_A3\u001b[0m/  \u001b[01;34mresults\u001b[0m/  train_pos.json  valid_pos.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIqhAEh-Wr44"
      },
      "source": [
        "model_file_path = './results/lstm/mnli'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qUM8SPRZL7e",
        "outputId": "d05cb98c-363e-45ff-aa36-8f2e7dcdaa8c"
      },
      "source": [
        "task.sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.legacy.data.field.Field at 0x7f56fa4edc90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYz6RSRQZr4m"
      },
      "source": [
        "import dill"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBzWXo9yYSIv"
      },
      "source": [
        "with open(os.path.join(model_file_path,'vocab'), 'wb') as file:\n",
        "  dill.dump(task.sent, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-j7z1e6OjGO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9fa70ee-18d2-412a-e208-69e689857111"
      },
      "source": [
        "## Zip the final model and all the required files, such as vocabulary\n",
        "# Replace USERID with your own, such as 2017CSZ8058\n",
        "!zip -r 2018EE10493_A_model.zip **\n",
        "\n",
        "## Upload it to Google drive and ensure that the testing notebook uses the correct link"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: data.zip (stored 0%)\n",
            "  adding: NLP_A3/ (stored 0%)\n",
            "  adding: NLP_A3/data/ (stored 0%)\n",
            "  adding: NLP_A3/data/train/ (stored 0%)\n",
            "  adding: NLP_A3/data/train/train.gold.txt (deflated 90%)\n",
            "  adding: NLP_A3/data/train/train.data.txt (deflated 63%)\n",
            "  adding: NLP_A3/data/validation/ (stored 0%)\n",
            "  adding: NLP_A3/data/validation/validation.data.txt (deflated 58%)\n",
            "  adding: NLP_A3/data/validation/validation.gold.txt (deflated 87%)\n",
            "  adding: results/ (stored 0%)\n",
            "  adding: results/lstm/ (stored 0%)\n",
            "  adding: results/lstm/mnli/ (stored 0%)\n",
            "  adding: results/lstm/mnli/best-lstm-mnli-params.pt (deflated 6%)\n",
            "  adding: results/lstm/mnli/train.log (deflated 78%)\n",
            "  adding: results/lstm/mnli/vocab (deflated 86%)\n",
            "  adding: train_pos.json (deflated 76%)\n",
            "  adding: valid_pos.json (deflated 74%)\n"
          ]
        }
      ]
    }
  ]
}